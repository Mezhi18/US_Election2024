---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - First author
  - Another author
thanks: "Code and data are available at: [https://github.com/Mezhi18/US_Election2024.git](https://github.com/Mezhi18/US_Election2024.git)."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format:
  pdf:
    bibliography: references.bib
    documentclass: article
    geometry: margin = 1in
number-sections: true
toc: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(dplyr)
library(janitor)
library(lubridate)
library(broom)
library(modelsummary)
library(rstanarm)
library(splines)
```


# Introduction

The 2024 United States presidential election represents a pivotal moment in the country's political landscape. As in previous elections, swing states are projected to play a critical role in determining the outcome. Swing states, which are characterized by their shifting voting patterns and balanced support for both major political parties, have historically been the focus of intense campaign efforts and polling analyses. Understanding the dynamics and voter preferences in these states is crucial to gaining insight into the broader electoral trends that could shape the nation's future.

In this paper we aim to analyze polling data related to the 2024 election, with a particular emphasis on the swing states. By examining various polls and identifying patterns in voter sentiment, we seek to uncover the factors that may influence voter behavior in these highly contested regions. Our analysis will explore demographic shifts, the impact of key issues, and the level of voter engagement across different swing states. Through a comprehensive statistical approach, we aim to contribute to the understanding of the evolving electoral landscape and provide meaningful insights into the forces shaping the 2024 presidential election.

Estimand paragraph

Results paragraph

Why it matters paragraph

Telegraphing paragraph: The remainder of this paper is structured as follows. @sec-data....

# Data {#sec-data}

## Overview

As our paper is about the 2024 United States federal election and more specifically we are looking at the polls and the polling data comparing the two candidates, former President Donald Trump and Vice President Kamala Harris, for the upcoming election. Our original Data set had over 16,000 unique entries from different pollsters, the business or Organization that conducts the poll. Each poll has two entries, one giving the Data for the polling opinions of Donald Trump and the second for Kamala Harris. We have acquired our polling data from @bibnotmadeyet.

As there are over 50 variables many of which are redundant to our paper we will only discuss those that we have kept in our clean data as they are the only ones we use in our analysis.

- `pollster`: Shows name of the Pollster that conducted the poll.
- `sample_size`: The number of people that participated in the specific poll.
- `state`: This variable tells us in which States the poll was conducted.
- `candidate_name`: This is the full name of the selected candidate.
- `pct`: This tells us the percentage of participants that intend to vote for the selected candidate.
- `start_date`: The date the pollster began conducting the poll.
- `end_date`: The date the pollster finished conducting the poll.

The variables that we have create are:

- `num_harris`: The number of participants that intend to vote for Kamala Harris.
- `end_date_num`: The number of days since the first poll since Harris announced her candidacy. 

Each pollster has a numeric grade from 1.0 to 3.0, which indicates the quality/ reliability of the respective pollster. Additionally, each pollster is also given a transparency score from 1.0 to 10.0 reflecting how 'transparent' the pollster is, or how much information is disclosed about its polls and methodology. It is important for pollsters to maintain high numeric grades and transparency scores because these metrics directly reflect the quality and reliability of their data. To ensure the highest level of accuracy in our predictions, we only include polls with a numeric grade of 1.5 or above and a transparency score of 6.0 or above.

Â¿ does this go into data cleaning ?

We use the statistical programming language R [@citeR].... Our data [@shelter].... Following @tellingstories, we consider...


## Methodology

This paper uses 'polls-of-polls' method to analyze and predict our outcome, which combines polls from multiple sources (pollsters). By combining results from various sources, this approach incorporates diverse perspectives, which helps to minimize individual biases and provides a more balanced view. Unlike relying on a single poll, which can be influenced by its own biases, the 'polls-of-polls' method enhances reliability and increases the overall validity of the results.


... explain how pollsters survey people, what their different methodologies are, and why these things are important for an accurate prediction...

## Measurement/ Data Visualization 

```{r}
#| include: false
#| warning: false
#| message: false

#Load and clean data
data <- read.csv("../data/01-raw_data/raw_data.csv") |>
  clean_names()

preddata <- data %>%
  filter(numeric_grade >1.5 & transparency_score >6) %>%
  select(pollster, sample_size, state, candidate_name, pct, start_date, end_date, pollscore, numeric_grade) %>%
  filter(!is.na(pollscore)) %>%
  filter(!is.na(state) & state != "")
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-supportstate
#| fig-cap: "Distribution of Support Percentage by State"

harrisdata <- preddata %>%
  filter(state %in% c("Georgia", "Arizona", "Nevada", "Michigan", "North Carolina", "Pennsylvania", "Wisconsin")) %>%
  filter(candidate_name == "Kamala Harris") %>%
  mutate(end_date = mdy(end_date)) %>%
  filter(end_date >= as.Date("2024-07-21")) %>%
  mutate(num_harris = round((pct / 100) * sample_size, 0))

ggplot(harrisdata, aes(x = reorder(state, pct, median), y = pct, fill = state)) +
  geom_boxplot() +
  coord_flip() +  # Flip for better readability
  labs(x = "State", y = "Support Percentage") +
  theme_minimal() +
  theme(legend.position = "none")
```

These are the statistics of each swing state by percetnage by state. 

```{r}
#| echo: false
#| warning: false
#| message: false

#### Plot data ####
base_plot <- ggplot(harrisdata, aes(x = end_date, y = pct)) +
  theme_classic() +
  labs(y = "Harris percent", x = "Date") +
    theme(legend.position = "bottom")

base_plot +
  geom_point(aes(color = state)) +
  geom_smooth() +
  theme(legend.position = "bottom")

ggplot(harrisdata, aes(x = end_date, y = pct, color = state)) +
  geom_point() +
  geom_smooth(se = TRUE) +
  facet_wrap(~ state, scales = "free_y") +
  labs(y = "Harris Percent", x = "Date", title = "Polling Trends for Kamala Harris by State") +
  theme_minimal()

```

## Outcome variables

Add graphs, tables and text. Use sub-sub-headings for each outcome variable or update the subheading to be singular.

Talk more about it.

And also planes (@fig-planes). (You can change the height and width, but don't worry about doing that until you have finished every other aspect of the paper - Quarto will try to make it look nice and the defaults usually work well once you have enough text.)

Talk way more about it.

## Predictor variables

Add graphs, tables and text.

Use sub-sub-headings for each outcome variable and feel free to combine a few into one if they go together naturally.

# Model

This is Model 1, follows Bayesian Model

We define our model as:

*define each vairable here*
```{=tex}
\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 + \beta_1 \times \mbox{State}_i + \beta_2 \times \mbox{Days Since First Poll}_i + \beta_3 \times \mbox{Pollster}_i \\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(0, 2.5) \\
\beta_2 &\sim \mbox{Normal}(0, 2.5) \\
\beta_3 &\sim \mbox{Exponential}(1)
\end{align}
```

```{r}
#| include: false
#| warning: false
#| message: false

# Turn end_date into quantifiable column, to use for model
harrisdata <- harrisdata %>%
  mutate(
    end_date_num = as.numeric(end_date - min(end_date)))

harrisdata <- harrisdata |>
  mutate(state = factor(state))

# Define Model
model <- cbind(num_harris, sample_size - num_harris) ~ (1 | state) + (1 | pollster) + (1 | end_date_num)

priors <- normal(0, 2.5, autoscale = TRUE)

bayesian_model <- stan_glmer(
  formula = model,
  data = harrisdata,
  family = binomial(link = "logit"),
  prior = priors,
  prior_intercept = priors,
  weights = harrisdata$numeric_grade,
  seed = 123,
  cores = 4,
  adapt_delta = 0.95)
```

```{r}
#| echo: false
#| warning: false
#| message: false

# Plot checks for Model
pp_check(bayesian_model)
plot(bayesian_model, pars = "(Intercept)", prob = 0.95)
modelsummary::modelsummary(
  list(
    "First model" = bayesian_model
  ),
  statistic = "mad",
  fmt = 2
)
```


This is Model 2 

```{r}
#| include: false
#| warning: false
#| message: false

spline_model <- stan_glm(
  pct ~ ns(end_date_num, df = 5) + pollster + state, 
  data = harrisdata,
  family = gaussian(),
  prior = normal(0, 5),
  prior_intercept = normal(50, 10),
  seed = 1234,
  iter = 2000,
  chains = 4,
  refresh = 0
)

# Summarize the model
summary(spline_model)

# Posterior predictive checks
pp_check(spline_model)

modelsummary::modelsummary(
  list(
    "First model" = spline_model
  ),
  statistic = "mad",
  fmt = 2
)
```

```{r}
#| echo: false
#| warning: false
#| message: false


harrisdata$pollster <- factor(harrisdata$pollster)
harrisdata$state <- factor(harrisdata$state)

new_data <- data.frame(
  end_date_num = seq(
    min(harrisdata$end_date_num),
    max(harrisdata$end_date_num),
    length.out = 100
  ),
  pollster = factor("YouGov", levels = levels(harrisdata$pollster)),
  state = factor("Georgia", levels = levels(harrisdata$state)))

posterior_preds <- posterior_predict(spline_model, newdata = new_data)

pred_summary <- new_data |>
  mutate(
    pred_mean = colMeans(posterior_preds),
    pred_lower = apply(posterior_preds, 2, quantile, probs = 0.025),
    pred_upper = apply(posterior_preds, 2, quantile, probs = 0.975)
  )

# Plot data and Bayesian model predictions with confidence ribbon
ggplot(harrisdata, aes(x = end_date_num, y = pct, color = state)) +
  geom_point() +
  geom_line(
    data = pred_summary,
    aes(x = end_date_num, y = pred_mean),
    color = "blue",
    inherit.aes = FALSE
  ) +
  geom_ribbon(
    data = pred_summary,
    aes(x = end_date_num, ymin = pred_lower, ymax = pred_upper),
    alpha = 0.2,
    inherit.aes = FALSE
  ) +
  labs(
    x = "Days since earliest poll",
    y = "Percentage",
    title = "Poll Percentage over Time"
  ) +
  facet_wrap(~ state, scales = "free_y") +
  theme_minimal()
```
The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.

### Model justification

What do we expect...
We expect a predictions in the high 40% based on our current data.


# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```

# Discussion

## Arizona {#sec-arizona}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Georgia {#sec-georgia}

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Michigan {#sec-michigan}

## Nevada {#sec-nevada}

## North Carolina {#sec-norcar}

## Michigan {#sec-michigan}

## Pennsylvania {#sec-penn}

## Wisconsin {#sec-Wisconsin}




## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

Why we choose NYT? - `numeric_grade` is 3 - `pollscore` is -1.5, a score of reliability called "Predictive Optimization of Latent skill Level in Surveys, Considering Overall Record, Empirically.", where negative numbers are better - `transparency_score` is 9, reflects pollsters transparency about their methodology (calculated based on how much information it discloses about its polls and weighted by recency) - `population_full` is 'rv', respondents are registered voters

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows...

```{r}
#| eval: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: false
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```

\newpage

# Appendix 1


# Appendix 2

Here we will be talking about how we would conduct a $100,000 surevey to gather data about this upcoming election to ensure there is minimal error in the data we are collecting and, our data represents the facts we want them to represent. 
Out of the \$100,000 we will allocate \$15000 for the necessary development and administration of the data. This means we will be spending this money to create the survey, and have a infrastructure in place to hold the large amount of data as well as cover any security fees to keep this data private and safe. Then we allocate $50000 to advertise the survey. We will need a large sample so we will be spending most of our budget for this. We will have a different urls for each advertisement we have so that we can incorporate this in our data to see which demographic is accessing the survey through which platform. The platforms to advertise this is through spotify, facebook, instagram, and some news networks. If possible we will try to use this money to get it endorsed by branches of government to show our reliability. Reliability is also what citizens respond to so this will get a lot of respondants. We will spend \$25,000 for modeling our data as they tend to be expensive with data this size. And the rest of the \$15,000 will be leftover cost for anything that we don't foresee. If there is any leftover we can add a survey participation price such as having a free 3 month trial for the platforms we mentioned above (that is if they have a subscription based membership). 

We will not be using telephone surveys because according to reserach [@idSurvey] we find the most people, don't pick up calls from unknown numbers and even the people that pick it up they are less inclined to answer the questions of the survey. 

Next to look at the actual contents of the survey. They can be accessed through this link:[Sample survey question](https://forms.office.com/Pages/ResponsePage.aspx?id=JsKqeAMvTUuQN7RtVsVSEL7G89G56QlHp_qqIPzGbJdUME9UQzJaWUlCVldZTktRV09UQ0Q3N0hQRi4u). There are 3 things that we were careful of when we were creating this survey. First thing we considered was transperency. People need the reassurence that the data that is being collected will not be used against them, and so we teel them what data we do collect and how the data we collect cannot be used to identify a person. The second thing we focus on is readability of the questions. We tried to make them as simple as possible using accessibile language, and tried to keep it short as well (@Tourangeau_Rips_Rasinski_2000). We also prioritised the size of the survey. We kept it to a short 11 questions that will tell us their political standing in the past and present. We know what current issues are important to them as well as their age group and the state they are from. This will help us gather data without inconveniencing the person. 

Finally we take a look at how we sample from the gathered data. We thought about random sampling but we were worried about having unequal proportions of the demographic we would advertise to. So if more people that go to facebook responded, that doesn't necessairly mean that people use facebook more than the other platforms. So we we decided to combine startified and cluster  sampling(@annurev). The idea is that we will choose based on the different platforms we advertise to first to have a cluster sampling method. And then we will sample the data by dividing the people by the certain aspects of the surevy, like if they are in different age groups or if they are registered voters. This is called Stratified Sampling (@annurev). Then we take a union of all the samples and we gather a group that we are able to build models based off of. 


\newpage

# References
